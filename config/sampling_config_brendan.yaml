# Sampling configuration for scripts/batch_rollout.py
#
# You can pass this file with:
#   python scripts/batch_rollout.py --config config/sampling_config.yaml \
#     --num-samples 2000 --input-data input_data.csv --time-range 2015 2035 --seed 123
#
# Notes:
# - Distributions supported: fixed, uniform, normal, lognormal, shifted_lognormal, beta, choice
# - normal supports optional min/max with clip_to_bounds: true
# - lognormal uses parameters in log space: mu, sigma; supports min/max with clip_to_bounds
# - beta draws in [0,1] then scales to [min,max]
# - choice supports optional probabilities p
# - 80% CI-first: For normal/lognormal, if ci80 is provided as [low, high] OR ci80_low/ci80_high are provided,
#   they TAKE PRECEDENCE over mean/sd (normal) or mu/sigma (lognormal). If both are present, mean/sd or mu/sigma
#   are ignored and a warning is emitted once per parameter.

seed: 32432432
initial_progress: 0.0
# If omitted on CLI, this is used; otherwise CLI --time-range overrides
time_range: [2012.0, 2045.0]

num_rollouts: 1000
input_data: input_data.csv
per_sample_timeout: 5

parameters:
  # Superhuman coder time horizon + extrapolation
  sc_time_horizon_minutes:
    dist: lognormal
    ci80: [62280, 2.5e12] # [6 work months, 2,000,000 work years]
  horizon_extrapolation_type:
    dist: fixed
    value: decaying doubling time

  # Gaps mode
  pre_gap_sc_time_horizon:
    dist: lognormal
    ci80: [23000, 14400000] # [0.2 work years, 125 work years]
  include_gap:
    dist: choice
    values: ["gap", "no gap"]
    p: [0.5, 0.5]
  gap_years:
    dist: lognormal
    ci80: [0.3, 7.5] 

  # Manual horizon fitting (used when horizon_extrapolation_type="decaying doubling time")
  present_day:
    dist: fixed
    value: 2025.6
  present_horizon:
    dist: fixed
    value: 26.0
  present_doubling_time:
    dist: lognormal
    ci80: [0.25, 0.667]
  doubling_difficulty_growth_rate:
    dist: normal
    ci80:
    - 0.8  # 1 - 0.2
    - 1.04  # 1 - (-0.04)

  # coding labor
  rho_coding_labor:
    dist: choice
    values: [-5, -2, -1]
    p: [0.25, 0.5, 0.25]

  # experiment capacity
  inf_compute_asymptote:
    dist: shifted_lognormal
    ci80: [25, 1000000]
    shift: 1.0
  inf_labor_asymptote:
    dist: shifted_lognormal
    ci80: [1, 200]
    shift: 1.0
  inv_compute_anchor_exp_cap:
    dist: shifted_lognormal
    ci80: [1, 5.4]
    shift: 1.0
  coding_automation_efficiency_slope:
    dist: lognormal
    ci80: [0.67, 6]  

# Below set to roughly match 10th percentile = 0.3, 90th = 0.8
  parallel_penalty:
    dist: beta
    alpha: 3.3
    beta: 2.8

  swe_multiplier_at_present_day:
    dist: shifted_lognormal
    ci80: [0.15, 2.4]
    shift: 1.0
  automation_interp_type:
    dist: fixed
    value: "linear"

  # Research taste dynamics
  ai_research_taste_at_superhuman_coder_sd:
    dist: normal
    ci80: [-3.5, 3.5]
  ai_research_taste_slope:
    dist: lognormal
    ci80: [0.5, 6.5]
  median_to_top_taste_multiplier:
    dist: shifted_lognormal
    ci80: [0.4, 12.7]
    shift: 1.0

  # Software scale / normalization
  r_software:
    dist: lognormal
    ci80: [0.7, 9]

  # Software efficiency growth rate in reference year (2023)
  software_progress_rate_at_reference_year:
    dist: lognormal
    ci80: [0.25, 2.5]  # 80% CI around default of 0.79

# Input time series uncertainty
# These parameters control the generation of time series data with uncertainty
time_series_parameters:
  constant_training_compute_growth_rate:
    dist: normal
    ci80: [0.55, 0.65]  # Training compute growth rate before slowdown
  slowdown_year:
    dist: uniform
    min: 2026.0
    max: 2030.0
  post_slowdown_training_compute_growth_rate:
    dist: normal
    ci80: [0.15, 0.35]  # Training compute growth rate after slowdown
